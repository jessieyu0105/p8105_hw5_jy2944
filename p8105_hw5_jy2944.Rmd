---
title: "p8105_hw5_jy2944"
author: "Jie Yu"
date: "11/6/2018"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)

library(tidyverse)
library(httr)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```


# Problem 1

### Import and tidy data

```{r p1_import_tidy}
# Start with a dataframe containing all file names: `list.files()`
study_df = tibble(file_names = list.files(path = "./data/problem1_data")) %>% 
  # Use `map()` to iterate over file names and read in data for each subject, and save the results in a new variable `data`
  mutate(data = map(.x = str_c("./data/problem1_data/", file_names), 
                    ~ read.csv(.x))) %>% 
  unnest() %>%
  # Begin Tidy
  janitor::clean_names() %>% 
  # remove str ".csv" in variable `file_names`
  mutate(file_names = str_replace(file_names, ".csv", "")) %>% 
  # manipulate file names to include study arm and subject ID
  separate(file_names, into = c("group", "subject_id"), sep = "_") %>% 
  # gather weekly observations
  gather(key = "week", value = "subject_data", week_1:week_8) %>% 
  mutate(
    # remove str "week_" in variable `week`
    week = str_replace(week, "week_", ""),
    subject_id = factor(subject_id),
    week = as.integer(week),
    group = factor(group)
  )
```

### Spaghetti plot

```{r p1_spaghetti_plot}
study_df %>% 
  mutate(group = recode(group, "con" = "control", "exp" = "experimental")) %>%
  ggplot(aes(x = week, y = subject_data, color = subject_id, group = subject_id)) +
  geom_point(alpha = 0.5) +
  geom_line() +
  facet_grid(. ~group) +
  labs(
    title = "Observations on each subject over time by control and experimental group",
    x = "Week",
    y = "Observation value",
    color = "Subject ID"
  ) 
```

Comments:

# Problem 2

### Import data
```{r p2_import_data, message = FALSE}
# Download the dataset from a Github repo: the file is too big to show on Github
homicide_data = GET("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  content("raw") %>%
  read_csv()
```

### Description of raw data

This raw data is gathered by the _Washington Post_, which records homicide cases in large U.S cities. The dataset is consisted of `r nrow(homicide_data)` observations and `r ncol(homicide_data)` variables. The variables contain information related to the reported date of each case, personal information of each victim (name, race, age, sex), information of the incident site (city, state, latitude, longitude), and the status of each case (`disposition`).


### Summary number of cases within cities

Then, We create a new variable `city_state` and summarize within cities to obtain the total number of homicide cases and number of unsolved homicide cases.

```{r p2_summary}
homicide_summary = homicide_data %>% 
  # create a `city_state` variable (e.g. â€œBaltimore, MDâ€) 
  mutate(city_state = str_c(city, ", ", state)) %>%
  group_by(city_state) %>% 
  summarise(
    # summarize total number of homicides
    total_cases = n(),
    # summarize number of unsolved homicides
    unsolved_cases = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
    )

head(homicide_summary, 10)
```

### Proportion test

For the city of Baltimore, MD, we use the `prop.test` function to estimate the proportion of homicides that are unsolved.

```{r p2_baltimore_test}
# filter the city "Baltimore, MD"
baltimore = homicide_summary %>% 
  filter(city_state == "Baltimore, MD")

# use `prop.test` to estimate the proportion of homicides that are unsolved
# save the output of prop.test as an R object
baltimore_prop = prop.test(baltimore$unsolved_cases, baltimore$total_cases)


baltimore_prop %>% 
  # turn a model object into a tidy tibble
  broom::tidy() %>% 
  # pull the estimated proportion and CI from the resulting tidy dataframe
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)
```


